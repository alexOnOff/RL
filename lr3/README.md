# Лабораторная работа №3. 
## Динамическое программирование 

### Обсуждение. Оценивание cтратегии (предcказание)

*1. Вывести уравнения, аналогичные (3), (4) для  qπ .*

```math
q_π(s,a) = \sum_{s^`, r}p(s^`,r|s,a)[r+γ v_π(s^`)]      (1)
```
```math
q_{k+1}(s) = \sum_{s^`, r}p(s^`,r|s,a)[r+γ v_k(s^`)] 
```

*2. Используя матрицу на рисунке выше для  vk  при  k=∞ , с помощью выведенных для  qπ  формул рассчитайте ценность действий через  vπ :  qπ(11,down),qπ(7,down),qπ(3,down) .*

Исходя из формулы (1) можно высчитать v_π(s`), но для этого необходимо знать значение γ. Найдем его:


 vπ(9) =  0,25 ( -1 - 20γ -1 - 18γ -1 - 14γ -1 + 0γ) = 0,25 (-3 - 52γ) = -1 - 13γ = -14

Тогда γ = 1

Значит ценность действия можно вычислить как

q_π(11,down) = 1 * (-1 + 1*0) = -1

q_π(7,down) = 1 * (-1 - 1\*(-14)) = -15

q_π(3,down) = 1 * (-1 - 1\*(-20)) = -21


### Обсуждение. Итерация по стратегиям

*1. Какое изменение необходимо добавить в алгоритм, чтобы он работал для случая с несколькими  π∗ ? Как следовало бы определить алгоритм итерации по стратегиям для ценностей действий?*

*2. Напишите программу, реализующую итерацию по стратегиям, и решите задачу об аренде машин.*

### Обсуждение. Итерация по ценности

*1. Почему у оптимальной политики в задаче игрока такая странная форма? В частности, для капитала  50  предлагается поставить все на одно подбрасывание, а для капитала  51  – нет. Почему такая стратегия хороша?*

*2. Реализуйте алгоритм итерации по ценности для задачи игрока и решите ее для  ph=[0.15;0.65]  с шагом  0.1 . Возможно, потребуется ввести два фиктивных состояния, соответствующих завершению с капиталом  0  и  100 , сопоставив им ценности  0  и  1 . Представьте свои результаты графически, как на рисунке выше. График функции ценности можно выводить один, конечный.*

