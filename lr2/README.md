# Лабораторная работа №2. 
## Конечные марковские процессы принятия решений - конечные МППР

В этой лабе мы поиграем в сеточный мир. 
Существует поле 5х5 (при желании в программе можно сделать мир и больше), при выходе за пределы мира штрафуем агента на -1, при хождении по миру награда - 0, при попадании в "волшебные" клетки переносим агента в указанную в клетке позицию и награждаем его большим пирожком.

Необходимо было проверить выполнение уравнения Беллмана на поле со следующими значениями функции ценности состояния:

    3.3    8.8    4.4    5.3    1.5
    1.5      3    2.3    1.9    0.5
    0.1    0.7    0.7    0.4   -0.4
     -1   -0.4   -0.4   -0.6   -1.2
    -1.9   -1.3   -1.2   -1.4     -2

При создании такого поля вызвать StudyOnce, то получим:

    3.3   8.83   4.43   5.36   1.48
    1.53   2.99   2.25   1.91   0.54
    0.04   0.77   0.67   0.36  -0.41
    -0.97  -0.45  -0.34  -0.59  -1.19
    -1.87  -1.33  -1.22  -1.42  -1.99

Вроде как этого было достаточно.

Но я реализовал еще и обучение по политике выбора рандомного хода (вверх, вниз, влево, вправо). Начальная матрица функции ценности состояния заполнена нулями. При одной итерации обучения получим:

    -0.5     10  -0.25      5   -0.5
    -0.25      0      0      0  -0.25
    -0.25      0      0      0  -0.25
    -0.25      0      0      0  -0.25
    -0.5  -0.25  -0.25  -0.25   -0.5

100 итераций:

    3.31   8.79   4.43   5.32   1.49
    1.52   2.99   2.25   1.91   0.55
    0.05   0.74   0.67   0.36   -0.4
    -0.97  -0.44  -0.35  -0.59  -1.18
    -1.86  -1.35  -1.23  -1.42  -1.98

10000 итераций:

    3.31   8.79   4.43   5.32   1.49
    1.52   2.99   2.25   1.91   0.55
    0.05   0.74   0.67   0.36   -0.4
    -0.97  -0.44  -0.35  -0.59  -1.18
    -1.86  -1.35  -1.23  -1.42  -1.98

При такой политике уравнение начнет сходиться уже при 100 итерациях "обучения" (Если это можно так назвать)

В целом, полсе этого обучения можно будет ставить на карту агента и запускать его в путешествие, политика будет такой: следующее состояние должно быть максимальным, среди всех возможных.
И все получится, как на картинке из методички. 


## Обсуждения.Основные определения

*1. Какова вероятность при такой диаграмме переходов закончить отпуск с суммарной наградой  r=0 ?*
![Текст с описанием картинки](/images/1.png)

Чтобы суммарная награда была 0, необходимо попасть в "Травму", так как только там существует отрицательная награда, которая компенсирует все предыдущие положительные.

Рассмотрим вероятности следующих путей:
Начало - 0.3 - пляж - 0.1 - спорт - 0.1 - Травма  = 0.003 

Начало - 0.7 - горы - 0.8 - спорт - 0.1 - Травма  = 0.056 

Вероятность суммарной награды 0 - 5,9%

Если посчитать все остальные исходы, то их вероятность будет 0,941


## Обсуждения.Граница между средой и агентом

*Достаточно ли структуры МППР для представления всех задач целенаправленного обучения? Можете ли вы придумать очевидные исключения?*

Нет, недостаточно, так как в реальном мире не существует таких "детерминированных" (с какой-либо долей вероятности) задач. Даже вышеописанная задача о роботе-пылесосе может быть усложнена внешними воздействиями, которые никак нельзя предугадать: магнитная буря, котик, который захотел сесть на робота, помехи от повышенного радиационного фона и тд. 

*Придумайте и разберите два примера задач, соблюдающих формализм МППР. Определите для каждого примера состояния, действия и вознаграждения.*

1. Эволюция роя зергов. Абатур работает на благо роя и пытается улучшать армию. Он может находиться в трех состояниях: выведение нового вида, улучшение существующего, восстановление сил.
Начинает всегда с выведения нового вида, из этого состояния есть 2 перехода: в текущее состояние с наградой 5 (продолжение выведения новых видов), и в "улучшение вида" с наградой 5.
Из состояния улучшения существующего вида он может перейти в то же состояние с вероятностью 0,9 и наградой 1 или же получить по башке от Керриган за самодеятельность и уйти на восстановление с вероятностью 0,1 и наградой -10. Из восстановления он с равной долей вероятности может пойти и выведение нового вида и на улучшение существующего вида с наградой 0 в обоих случаях.

2. Игрок в SC2 может единовременно строить базу, строить войска, воевать и бездействовать. Постройка базы и войска тратит минералы, боевые действия и бездействие накапливает минералы. Задача игрока грамотно определять свои действия, чтобы его ресурсы не уходили в ноль и не были слишком высокими. 
Соответственно, если минералов много, положительной наградой будет выбор строительства, отрицательной управление армией и бездействие. И наоборот при дефиците ресурсов.

*Рассмотрим задачу о вождении автомобиля. Можно было бы определить действия в терминах акселератора, руля и тормозов, т. е. тех мест, где Ваше тело входит в контакт с машиной. А можно было бы поместить их дальше снаружи – например, там, где шина контактирует с дорогой, считая вашими действиями крутящий момент на колесе. Или поместить их дальше внутри – скажем, там, где ваш мозг вступает в контакт с телом, и считать действиями сокращения мускулов, управляющих вашими конечностями. Или можно перейти на совсем уж высокий уровень и сказать, что ваше действие – это выбор места назначения. Какой уровень правильный, т. е. где нужно было бы провести границу между агентом и окружающей средой? На каком основании следует предпочесть одно положение границы другому? Существует ли фундаментальная причина предпочесть одно положение другому или это свободный выбор?*

Правильного уровня нет, так как любую задачу можно рассматривать с любого уровня абстракции.
Одно положение границы другому необходимо предпочесть другому на основании конкретной задачи. Нет смысла рассматривать мозговые процессы водителя и расположение солнечной системы, если требуется рассчитать тормозной путь по конкретной дороге.
Я считаю, что фундаментальных причин нет и все зависит от разработчика и его метода решения.

*Постройте такую же таблицу, как в примере 3,но для  p(s′,r|s,a) . В ней должны быть столбцы s, a, s', r и  p(s′,r|s,a)  и по одной строке для каждой четверки такой, что  p(s′,r|s,a)>0 .*


| s     |a | s′ | r |  p(s′, r/s, a) |
|-------|--|----|---|----------------|
high|search|high|r_search| α
high|search|low|r_search|1 - α
low|search|high|-3|1 - β
low|search|low|r_search| β
high|wait|high|r_wait|1
low|wait|low|r_wait|1
low|recharge|high|0|1

Точно ли?


## Обсуждения.Доход и эпизоды

*Вы проектируете робота для прохождения лабиринта и решили давать вознаграждение +1 за выход из лабиринта и 0 в остальных случаях. Задача естественно распадается на эпизоды – последовательные прохождения лабиринта, поэтому вы решили рассматривать ее как эпизодическую задачу, в которой цель – максимизировать ожидаемое полное вознаграждение(ожидаемый доход) при  γ=1 . После некоторого времени тренировки обучающегося агента в лабиринте, вы обнаружили полное отсутствие прогресса. Что не так? Достаточно ли доходчиво вы сообщили агенту, чего именно он должен достичь?*

Нет, недостаточно. Робот в таком случае может спокойно ходить на месте. Ему необходимо давать отрицательную награду, чтобы он стремился найти положительный.

*Предположим, что  γ=0.5 , последовательность полученных вознаграждений имеет вид:  R1=–1 ,  R2=2 ,  R3=6 ,  R4=3 ,  R5=2  и  T=5 . Чему равны  G0 ,  G1 , …,  G5 ? Указание: идите от конца к началу.*

G5 = 0 (G должен отставать на шаг от R)

G4 = R5 + y*G5 = 2

G3 = R4 + y*G4 = 3 + 0.5*2 = 4

G2 = R3 + y*G3 = 6 + 2 = 8

G1 = R2 + y*G2 = 2 + 4 = 6

G0 = R1 + y*G1 = -1 + 3 = 2


*Предположим, что  γ=0.9  и последовательность вознаграждений начинается с  R1=2 , за которым следует бесконечная последовательность чисел 7. Чему равны  G0  и  G1 ?*

G2 = R3 / (1 - γ) = 7 / (1 - 0.9) = 70

G1 = R2 + y*G2 =7 + 0.9 * 70 = 70

G0 = R1 + y*G1 = 2 + 0.9\*70 = 65

## Обсуждения.Оптимальные стратегии и оптимальные функции ценности

*Для примера сеточного мира проверьте выполнение уравнений оптимальности Беллмана для  v∗π . Ответ нужно вывести в виде матрицы (как в задании после примера №5*)

После обучения оптимальной стратегией (manager->StudyOptimal(10000)) получились следующие значения v∗π:


    21.98  24.42  21.98  19.42  17.48
    19.78  21.98  19.78   17.8  16.02
    17.8  19.78   17.8  16.02  14.42
    16.02   17.8  16.02  14.42  12.98
    14.42  16.02  14.42  12.98  11.68

*В приведенном ниже непрерывном МППР принимается единственное решение в верхнем состоянии путем выбора действия  left  или  right . За каждое действие на рисунке приведены детерминированные вознаграждения:  +1,0  и  0,+2 . Таким образом, существуют две стратегии  πleft  и  πright . Программно определите, какая стратегия будет оптимальной при разных  γ=[0,1]  с шагом  0.1 . Подсказка: оценивать стоит ожидаемый доход.*

Left Rule
gamma = 0 Cost = 1
gamma = 0.1 Cost = 1.0101
gamma = 0.2 Cost = 1.04167
gamma = 0.3 Cost = 1.0989
gamma = 0.4 Cost = 1.19048
gamma = 0.5 Cost = 1.33333
gamma = 0.6 Cost = 1.5625
gamma = 0.7 Cost = 1.96078
gamma = 0.8 Cost = 2.77778
gamma = 0.9 Cost = 5.26316
gamma = 1 Cost = 5002.98

Right Rule
gamma = 0 Cost = 0
gamma = 0.1 Cost = 0.20202
gamma = 0.2 Cost = 0.416667
gamma = 0.3 Cost = 0.659341
gamma = 0.4 Cost = 0.952381
gamma = 0.5 Cost = 1.33333
gamma = 0.6 Cost = 1.875
gamma = 0.7 Cost = 2.7451
gamma = 0.8 Cost = 4.44445
gamma = 0.9 Cost = 9.47369
gamma = 1 Cost = 10006

До gamma = 0.5 стоит брать первую (левую) стратегию, после 0.5 - вторую (правую)